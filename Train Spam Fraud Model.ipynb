{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb17354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arman\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc1fc3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources checked/downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources if not already downloaded\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    print(\"NLTK stopwords downloaded.\")\n",
    "\n",
    "try:\n",
    "    WordNetLemmatizer()\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4') # Open Multilingual Wordnet\n",
    "    print(\"NLTK wordnet and omw-1.4 downloaded.\")\n",
    "\n",
    "print(\"NLTK resources checked/downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c193a",
   "metadata": {},
   "source": [
    "Text Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6897dd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing function defined.\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # Lowercasing\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text) # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text) # Remove numbers\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words]) # Remove stopwords\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()]) # Lemmatization\n",
    "    return text\n",
    "\n",
    "print(\"Preprocessing function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a631c15",
   "metadata": {},
   "source": [
    "Load and Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a41d7487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded spam.csv. Shape: (5574, 2)\n",
      "Spam DataFrame Head:\n",
      "    label                                            message\n",
      "0  normal  \"Go until jurong point, crazy.. Available only...\n",
      "1  normal                   Ok lar... Joking wif u oni...,,,\n",
      "2   fraud  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3  normal  U dun say so early hor... U c already then say...\n",
      "4  normal  \"Nah I don't think he goes to usf, he lives ar...\n",
      "Error loading fraud_call.file: Custom line terminators not supported in python parser (yet)\n",
      "\n",
      "Only spam_df loaded. Combined DataFrame Shape: (5574, 2)\n",
      "\n",
      "Combined DataFrame Sample:\n",
      "       label                                            message\n",
      "4387  normal  \", im .. On the snowboarding trip. I was wonde...\n",
      "3993  normal  \"Dizzamn, aight I'll ask my suitemates when I ...\n",
      "2249  normal                 will you like to be spoiled? :),,,\n",
      "2273  normal  \"Haha awesome, I've been to 4u a couple times....\n",
      "2340  normal  Cheers for the message Zogtorius. IåÕve been s...\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration for paths ---\n",
    "spam_path = 'data/spam.csv'\n",
    "fraud_path = 'data/fraud_call.file' # Corrected file name!\n",
    "\n",
    "# --- Load spam.csv ---\n",
    "try:\n",
    "    # Based on the screenshot: it's comma-separated, has a header,\n",
    "    # and messages contain commas but are NOT quoted.\n",
    "    # The most robust way for this specific structure\n",
    "    # is often to read it line by line and manually split.\n",
    "    \n",
    "    data = []\n",
    "    with open(spam_path, 'r', encoding='latin-1') as f:\n",
    "        header_skipped = False\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if not header_skipped: # Skip the header line (v1,v2)\n",
    "                header_skipped = True\n",
    "                continue\n",
    "\n",
    "            line = line.strip()\n",
    "            if not line: # Skip empty lines\n",
    "                continue\n",
    "\n",
    "            # Find the index of the first comma after the 'label' (v1)\n",
    "            # This assumes 'v1' (label) does not contain commas.\n",
    "            first_comma_idx = line.find(',')\n",
    "            if first_comma_idx != -1:\n",
    "                label = line[:first_comma_idx].strip()\n",
    "                message = line[first_comma_idx+1:].strip()\n",
    "                data.append({'label': label, 'message': message})\n",
    "            else:\n",
    "                # This could happen if a line only has a label and no message, or is just malformed\n",
    "                print(f\"Warning: Line {line_num} in spam.csv has no comma to split label and message: '{line}' - skipping.\")\n",
    "    \n",
    "    spam_df = pd.DataFrame(data)\n",
    "    spam_df['label'] = spam_df['label'].map({'ham': 'normal', 'spam': 'fraud'}) # Standardize labels\n",
    "    print(f\"Loaded spam.csv. Shape: {spam_df.shape}\")\n",
    "    print(\"Spam DataFrame Head:\")\n",
    "    print(spam_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading spam.csv: {e}\")\n",
    "    spam_df = pd.DataFrame() # Create empty DataFrame to avoid errors later\n",
    "\n",
    "\n",
    "# --- Load fraud_call.file ---\n",
    "try:\n",
    "    # Based on screenshot: tab-separated, no header.\n",
    "    # Use on_bad_lines='skip' to gracefully handle malformed lines like line 456\n",
    "    # The `engine='python'` can sometimes be more lenient with line parsing as well.\n",
    "    fraud_df = pd.read_csv(fraud_path, sep='\\t', encoding='latin-1', header=None, names=['label', 'message'],\n",
    "                           lineterminator='\\n', # Ensure correct line ending detection\n",
    "                           on_bad_lines='skip',  # Skip lines that don't match expected fields (e.g., line 456 with 3 fields)\n",
    "                           engine='python') # Python engine is more flexible for bad lines/complex cases\n",
    "    print(f\"\\nLoaded fraud_call.file. Shape: {fraud_df.shape}\")\n",
    "    print(\"Fraud DataFrame Head:\")\n",
    "    print(fraud_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading fraud_call.file: {e}\")\n",
    "    fraud_df = pd.DataFrame() # Create empty DataFrame to avoid errors later\n",
    "\n",
    "# --- Combine datasets ---\n",
    "if not spam_df.empty and not fraud_df.empty:\n",
    "    combined_df = pd.concat([spam_df, fraud_df], ignore_index=True)\n",
    "    print(f\"\\nCombined DataFrame Shape: {combined_df.shape}\")\n",
    "    print(\"Combined DataFrame Label Distribution:\")\n",
    "    print(combined_df['label'].value_counts())\n",
    "elif not spam_df.empty:\n",
    "    combined_df = spam_df\n",
    "    print(f\"\\nOnly spam_df loaded. Combined DataFrame Shape: {combined_df.shape}\")\n",
    "elif not fraud_df.empty:\n",
    "    combined_df = fraud_df\n",
    "    print(f\"\\nOnly fraud_call.file loaded. Combined DataFrame Shape: {combined_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nNo data loaded successfully. Check file paths and contents.\")\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "# Display some combined data\n",
    "if not combined_df.empty:\n",
    "    print(\"\\nCombined DataFrame Sample:\")\n",
    "    print(combined_df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e8440",
   "metadata": {},
   "source": [
    "Applying Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de38a0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying preprocessing to messages...\n",
      "Preprocessing complete!\n",
      "\n",
      "Combined DataFrame with processed messages (sample):\n",
      "                                                message  \\\n",
      "2665                   He remains a bro amongst bros,,,   \n",
      "3056  EASTENDERS TV Quiz. What FLOWER does DOT compa...   \n",
      "2601  \"As usual..iam fine, happy &amp; doing well..:...   \n",
      "4141  Leave it wif me lar... ÌÏ wan to carry meh so ...   \n",
      "5544        Armand says get your ass over to epsilon,,,   \n",
      "\n",
      "                                      processed_message   label  \n",
      "2665                           remains bro amongst bros  normal  \n",
      "3056  eastenders tv quiz flower dot compare violet e...   fraud  \n",
      "2601                       usualiam fine happy amp well  normal  \n",
      "4141  leave wif lar ìï wan carry meh heavy da num fa...  normal  \n",
      "5544                          armand say get as epsilon  normal  \n"
     ]
    }
   ],
   "source": [
    "if not combined_df.empty:\n",
    "    print(\"Applying preprocessing to messages...\")\n",
    "    combined_df['processed_message'] = combined_df['message'].apply(preprocess_text)\n",
    "    print(\"Preprocessing complete!\")\n",
    "    print(\"\\nCombined DataFrame with processed messages (sample):\")\n",
    "    print(combined_df[['message', 'processed_message', 'label']].sample(5))\n",
    "else:\n",
    "    print(\"No data to preprocess.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b3de4",
   "metadata": {},
   "source": [
    "Preparing Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eefec1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 4457\n",
      "Testing data size: 1115\n",
      "Data split into training and testing sets.\n"
     ]
    }
   ],
   "source": [
    "if not combined_df.empty:\n",
    "    # Drop rows with missing processed messages or labels\n",
    "    combined_df = combined_df.dropna(subset=['processed_message', 'label'])\n",
    "\n",
    "    # Features and labels\n",
    "    X = combined_df['processed_message']\n",
    "    y = combined_df['label']\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"Training data size: {len(X_train)}\")\n",
    "    print(f\"Testing data size: {len(X_test)}\")\n",
    "    print(\"Data split into training and testing sets.\")\n",
    "else:\n",
    "    print(\"No data available for training. Check previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b6971",
   "metadata": {},
   "source": [
    "Initializing and Training TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e09c1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF Vectorizer on training data...\n",
      "TF-IDF Vectorizer fitted.\n",
      "Shape of vectorized training data: (4457, 5000)\n",
      "Shape of vectorized test data: (1115, 5000)\n"
     ]
    }
   ],
   "source": [
    "if 'X_train' in locals() and not X_train.empty:\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "    print(\"Fitting TF-IDF Vectorizer on training data...\")\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    print(\"TF-IDF Vectorizer fitted.\")\n",
    "    print(f\"Shape of vectorized training data: {X_train_vectorized.shape}\")\n",
    "\n",
    "    # Transform test data\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "    print(f\"Shape of vectorized test data: {X_test_vectorized.shape}\")\n",
    "else:\n",
    "    print(\"Training data not available for vectorization. Check previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30acd39",
   "metadata": {},
   "source": [
    "Initializing and Training the Classifier (LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cbef360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LinearSVC Classifier...\n",
      "Classifier training complete!\n",
      "\n",
      "--- Model Evaluation ---\n",
      "Accuracy: 0.9821\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       fraud       1.00      0.87      0.93       149\n",
      "      normal       0.98      1.00      0.99       966\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.93      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'X_train_vectorized' in locals():\n",
    "    classifier = LinearSVC(dual='auto', random_state=42) # Set dual='auto' to silence FutureWarning\n",
    "\n",
    "    print(\"\\nTraining LinearSVC Classifier...\")\n",
    "    classifier.fit(X_train_vectorized, y_train)\n",
    "    print(\"Classifier training complete!\")\n",
    "\n",
    "    # Make predictions and evaluate\n",
    "    y_pred = classifier.predict(X_test_vectorized)\n",
    "    print(\"\\n--- Model Evaluation ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "else:\n",
    "    print(\"Vectorized training data not available for classifier training. Check previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfb0d4",
   "metadata": {},
   "source": [
    " Saving the Trained Vectorizer and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1102447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: models\n",
      "\n",
      "Model and Vectorizer saved to 'models' directory:\n",
      "- models\\tfidf_vectorizer.pkl\n",
      "- models\\text_classifier_model.pkl\n",
      "\n",
      "--- Training and Model Saving Complete! ---\n",
      "You can now use 'app.py' to load these models and run the Streamlit GUI.\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    print(f\"Created directory: {model_dir}\")\n",
    "\n",
    "if 'vectorizer' in locals() and 'classifier' in locals():\n",
    "    vectorizer_path = os.path.join(model_dir, 'tfidf_vectorizer.pkl')\n",
    "    model_path = os.path.join(model_dir, 'text_classifier_model.pkl')\n",
    "\n",
    "    joblib.dump(vectorizer, vectorizer_path)\n",
    "    joblib.dump(classifier, model_path)\n",
    "    print(f\"\\nModel and Vectorizer saved to '{model_dir}' directory:\")\n",
    "    print(f\"- {vectorizer_path}\")\n",
    "    print(f\"- {model_path}\")\n",
    "else:\n",
    "    print(\"Vectorizer or Classifier not trained. Cannot save models.\")\n",
    "\n",
    "print(\"\\n--- Training and Model Saving Complete! ---\")\n",
    "print(\"You can now use 'app.py' to load these models and run the Streamlit GUI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d29a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
